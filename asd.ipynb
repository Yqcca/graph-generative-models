{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /Users/ca/molecule-datasets/250k_rndm_zinc_drugs_clean_3.csv:  50%|█████     | 249456/498911 [00:03<00:03, 71388.20it/s]\n",
      "Constructing molecules from SMILES: 100%|██████████| 249455/249455 [07:05<00:00, 586.82it/s] \n"
     ]
    }
   ],
   "source": [
    "from gatv2 import GATV2\n",
    "\n",
    "import torch\n",
    "from torchdrug import datasets\n",
    "\n",
    "#dataset = datasets.ClinTox(\"~/molecule-datasets/\")\n",
    "dataset = datasets.ZINC250k(\"~/molecule-datasets/\", kekulize=True, atom_feature=\"symbol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchdrug import core, models, tasks\n",
    "from torch import nn, optim\n",
    "from collections import defaultdict\n",
    "from torchdrug.layers import distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import checkpoint\n",
    "from torch_geometric.utils import degree\n",
    "from torch_scatter import scatter\n",
    "from torchdrug import layers, core\n",
    "from torchdrug.layers import MessagePassingBase\n",
    "\n",
    "\n",
    "# PNA aggregators\n",
    "\n",
    "def aggregate_sum(src: Tensor, index: Tensor, dim_size: Optional[int]):\n",
    "    return scatter(src, index, 0, None, dim_size, reduce='sum')\n",
    "\n",
    "\n",
    "def aggregate_mean(src: Tensor, index: Tensor, dim_size: Optional[int]):\n",
    "    return scatter(src, index, 0, None, dim_size, reduce='mean')\n",
    "\n",
    "\n",
    "def aggregate_min(src: Tensor, index: Tensor, dim_size: Optional[int]):\n",
    "    return scatter(src, index, 0, None, dim_size, reduce='min')\n",
    "\n",
    "\n",
    "def aggregate_max(src: Tensor, index: Tensor, dim_size: Optional[int]):\n",
    "    return scatter(src, index, 0, None, dim_size, reduce='max')\n",
    "\n",
    "\n",
    "def aggregate_var(src, index, dim_size):\n",
    "    mean = aggregate_mean(src, index, dim_size)\n",
    "    mean_squares = aggregate_mean(src * src, index, dim_size)\n",
    "    return mean_squares - mean * mean\n",
    "\n",
    "\n",
    "def aggregate_std(src, index, dim_size):\n",
    "    return torch.sqrt(torch.relu(aggregate_var(src, index, dim_size)) + 1e-5)\n",
    "\n",
    "\n",
    "# PNA scalers\n",
    "\n",
    "def scale_identity(src: Tensor, deg: Tensor, avg_deg: Dict[str, float]):\n",
    "    return src\n",
    "\n",
    "\n",
    "def scale_amplification(src: Tensor, deg: Tensor, avg_deg: Dict[str, float]):\n",
    "    return src * (torch.log(deg + 1) / avg_deg['log'])\n",
    "\n",
    "\n",
    "def scale_attenuation(src: Tensor, deg: Tensor, avg_deg: Dict[str, float]):\n",
    "    scale = avg_deg['log'] / torch.log(deg + 1)\n",
    "    scale[deg == 0] = 1\n",
    "    return src * scale\n",
    "\n",
    "\n",
    "def scale_linear(src: Tensor, deg: Tensor, avg_deg: Dict[str, float]):\n",
    "    return src * (deg / avg_deg['linear'])\n",
    "\n",
    "\n",
    "def scale_inverse_linear(src: Tensor, deg: Tensor, avg_deg: Dict[str, float]):\n",
    "    scale = avg_deg['linear'] / deg\n",
    "    scale[deg == 0] = 1\n",
    "    return src * scale\n",
    "\n",
    "\n",
    "AGGREGATORS = {\n",
    "    'sum': aggregate_sum,\n",
    "    'mean': aggregate_mean,\n",
    "    'min': aggregate_min,\n",
    "    'max': aggregate_max,\n",
    "    'var': aggregate_var,\n",
    "    'std': aggregate_std,\n",
    "}\n",
    "\n",
    "SCALERS = {\n",
    "    'identity': scale_identity,\n",
    "    'amplification': scale_amplification,\n",
    "    'attenuation': scale_attenuation,\n",
    "    'linear': scale_linear,\n",
    "    'inverse_linear': scale_inverse_linear\n",
    "}\n",
    "\n",
    "\n",
    "class PNALayer(MessagePassingBase):\n",
    "    \"\"\"\n",
    "    The Principal Neighbourhood Aggregation graph convolution operator from\n",
    "    `Principal Neighbourhood Aggregation for Graph Nets`_.\n",
    "\n",
    "    .. _Principal Neighbourhood Aggregation for Graph Nets:\n",
    "        https://arxiv.org/pdf/2004.05718.pdf\n",
    "\n",
    "    Parameters:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        aggregators (list of str): Set of aggregation function identifiers,\n",
    "            namely \"sum\", \"mean\", \"min\", \"max\", \"var\" and \"std\".\n",
    "        scalers: (list of str): Set of scaling function identifiers, namely\n",
    "            \"identity\", \"amplification\", \"attenuation\", \"linear\" and \"inverse_linear\".\n",
    "        deg (Tensor): Histogram of in-degrees of nodes in the training set, used by scalers to normalise.\n",
    "        edge_dim (int, optional): Edge feature dimensionality (in case there are any).\n",
    "        towers (int, optional): Number of towers.\n",
    "        pre_layers (int, optional): Number of transformation layers before aggregation.\n",
    "        post_layers (int, optional): Number of transformation layers after aggregation.\n",
    "        divide_input (bool, optional): Whether the input features should be split between towers or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 aggregators: List[str], scalers: List[str], deg: Tensor,\n",
    "                 edge_dim: Optional[int] = None, towers: int = 1,\n",
    "                 pre_layers: int = 1, post_layers: int = 1,\n",
    "                 divide_input: bool = False):\n",
    "        super(PNALayer, self).__init__()\n",
    "\n",
    "        if divide_input:\n",
    "            assert in_channels % towers == 0\n",
    "        assert out_channels % towers == 0\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.aggregators = [AGGREGATORS[aggr] for aggr in aggregators]\n",
    "        self.scalers = [SCALERS[scaler] for scaler in scalers]\n",
    "        self.edge_dim = edge_dim\n",
    "        self.towers = towers\n",
    "        self.divide_input = divide_input\n",
    "\n",
    "        self.F_in = in_channels // towers if divide_input else in_channels\n",
    "        self.F_out = self.out_channels // towers\n",
    "\n",
    "        deg = deg.to(torch.float)\n",
    "        total_no_vertices = deg.sum()\n",
    "        bin_degrees = torch.arange(len(deg))\n",
    "        self.avg_deg: Dict[str, float] = {\n",
    "            'linear': ((bin_degrees * deg).sum() / total_no_vertices).item(),\n",
    "            'log': (((bin_degrees + 1).log() * deg).sum() / total_no_vertices).item(),\n",
    "            'exp': ((bin_degrees.exp() * deg).sum() / total_no_vertices).item(),\n",
    "        }\n",
    "\n",
    "        if self.edge_dim is not None:\n",
    "            self.edge_encoder = nn.Linear(edge_dim, self.F_in)\n",
    "\n",
    "        self.pre_mlps = nn.ModuleList()\n",
    "        self.post_mlps = nn.ModuleList()\n",
    "        for _ in range(towers):\n",
    "            pre_modules = [nn.Linear((3 if edge_dim else 2) * self.F_in, self.F_in)]\n",
    "            for _ in range(pre_layers - 1):\n",
    "                pre_modules += [nn.ReLU()]\n",
    "                pre_modules += [nn.Linear(self.F_in, self.F_in)]\n",
    "            self.pre_mlps.append(nn.Sequential(*pre_modules))\n",
    "\n",
    "            in_channels = (len(aggregators) * len(scalers) + 1) * self.F_in\n",
    "            post_modules = [nn.Linear(in_channels, self.F_out)]\n",
    "            for _ in range(post_layers - 1):\n",
    "                post_modules += [nn.ReLU()]\n",
    "                post_modules += [nn.Linear(self.F_out, self.F_out)]\n",
    "            self.post_mlps.append(nn.Sequential(*post_modules))\n",
    "\n",
    "        self.update_mlp = nn.Linear(out_channels, out_channels)\n",
    "\n",
    "    def message(self, graph, input):\n",
    "        node_in = graph.edge_list[:, 0]\n",
    "        node_out = graph.edge_list[:, 1]\n",
    "        if graph.num_edge:\n",
    "            h = torch.cat([input[node_in], input[node_out]], dim=-1)\n",
    "            message = [mlp(h[:, i]) for i, mlp in enumerate(self.pre_mlps)]\n",
    "        else:\n",
    "            message = [torch.zeros(0, self.F_in, device=graph.device) for _ in range(self.towers)]\n",
    "        return torch.stack(message, dim=1)\n",
    "\n",
    "    def aggregate(self, graph, message):\n",
    "        node_out = graph.edge_list[:, 1]\n",
    "        edge_weight = graph.edge_weight.view(-1, 1, 1)\n",
    "        edge_weight = edge_weight.repeat(1, self.towers, 1)\n",
    "        update = [aggr(message * edge_weight, node_out, graph.num_node) for aggr in self.aggregators]\n",
    "        update = torch.cat(update, dim=-1)\n",
    "        deg = degree(node_out, graph.num_node, dtype=message.dtype).view(-1, 1, 1)\n",
    "        update = [scaler(update, deg, self.avg_deg) for scaler in self.scalers]\n",
    "        return torch.cat(update, dim=-1)\n",
    "\n",
    "    def combine(self, input, update):\n",
    "        output = torch.cat([input, update], dim=-1)\n",
    "        output = [mlp(output[:, i]) for i, mlp in enumerate(self.post_mlps)]\n",
    "        output = torch.cat(output, dim=1)\n",
    "        return self.update_mlp(output)\n",
    "\n",
    "    def forward(self, graph, input):\n",
    "        if self.divide_input:\n",
    "            input = input.view(-1, self.towers, self.F_in)\n",
    "        else:\n",
    "            input = input.view(-1, 1, self.F_in).repeat(1, self.towers, 1)\n",
    "\n",
    "        if self.gradient_checkpoint:\n",
    "            update = checkpoint.checkpoint(self._message_and_aggregate, *graph.to_tensors(), input)\n",
    "        else:\n",
    "            update = self.message_and_aggregate(graph, input)\n",
    "        output = self.combine(input, update)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, towers={self.towers})')\n",
    "\n",
    "\n",
    "class PNA(nn.Module, core.Configurable):\n",
    "    \"\"\"\n",
    "    Graph Substructure Network proposed in `Improving Graph Neural Network Expressivity\n",
    "    via Subgraph Isomorphism Counting`_.\n",
    "\n",
    "    This implements the GSN-v (vertex-count) variant in the original paper.\n",
    "\n",
    "    .. _Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting:\n",
    "        https://arxiv.org/pdf/2006.09252.pdf\n",
    "\n",
    "    Parameters:\n",
    "        input_dim (int): input dimension\n",
    "        hidden_dim (int): hidden dimension\n",
    "        edge_input_dim (int): dimension of edge features\n",
    "        num_relation (int): number of relations\n",
    "        num_layer (int): number of hidden layers\n",
    "        aggregators (list of str): set of aggregation function identifiers,\n",
    "            namely \"sum\", \"mean\", \"min\", \"max\", \"var\" and \"std\"\n",
    "        scalers: (list of str): set of scaling function identifiers, namely\n",
    "            \"identity\", \"amplification\", \"attenuation\", \"linear\" and \"inverse_linear\"\n",
    "        deg (Tensor): histogram of in-degrees of nodes in the training set, used by scalers to normalise\n",
    "        num_tower (int, optional): number of towers\n",
    "        num_pre_layer (int, optional): number of MLP layers in each pre-transformation network\n",
    "        num_post_layer (int, optional): number of MLP layers in each post-transformation network\n",
    "        divide_input (bool, optional): whether the input features should be split between towers or not\n",
    "        short_cut (bool, optional): use short cut or not\n",
    "        batch_norm (bool, optional): apply batch normalization or not\n",
    "        activation (str or function, optional): activation function\n",
    "        concat_hidden (bool, optional): concat hidden representations from all layers as output\n",
    "        readout (str, optional): readout function. Available functions are ``sum`` and ``mean``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, edge_input_dim, num_relation, num_layer, aggregators, scalers, deg,\n",
    "                 num_tower=1, num_pre_layer=1, num_post_layer=1, divide_input=False, short_cut=False, batch_norm=False,\n",
    "                 activation='relu', concat_hidden=False, readout='sum'):\n",
    "        super(PNA, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.edge_input_dim = edge_input_dim\n",
    "        if concat_hidden:\n",
    "            feature_dim = hidden_dim * num_layer\n",
    "        else:\n",
    "            feature_dim = hidden_dim\n",
    "        self.output_dim = feature_dim\n",
    "        self.num_relation = num_relation\n",
    "        self.num_layer = num_layer\n",
    "        self.short_cut = short_cut\n",
    "        self.concat_hidden = concat_hidden\n",
    "\n",
    "        self.node_encoder = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layer):\n",
    "            self.layers.append(\n",
    "                PNALayer(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers,\n",
    "                         deg=deg, edge_dim=edge_input_dim, towers=num_tower, pre_layers=num_pre_layer,\n",
    "                         post_layers=num_post_layer, divide_input=divide_input))\n",
    "\n",
    "        if batch_norm:\n",
    "            self.batch_norm = nn.BatchNorm1d(hidden_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = getattr(F, activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "\n",
    "        if readout == 'sum':\n",
    "            self.readout = layers.SumReadout()\n",
    "        elif readout == 'mean':\n",
    "            self.readout = layers.MeanReadout()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown readout {readout}')\n",
    "\n",
    "    def forward(self, graph, input, all_loss=None, metric=None):\n",
    "        \"\"\"\n",
    "        Compute the node representations and the graph representation(s).\n",
    "        Parameters:\n",
    "            graph (Graph): :math:`n` graph(s)\n",
    "            input (Tensor): input node representations\n",
    "            all_loss (Tensor, optional): if specified, add loss to this tensor\n",
    "            metric (dict, optional): if specified, output metrics to this dict\n",
    "        Returns:\n",
    "            dict with ``node_feature`` and ``graph_feature`` fields:\n",
    "                node representations of shape :math:`(|V|, d)`, graph representations of shape :math:`(n, d)`\n",
    "        \"\"\"\n",
    "        hiddens = []\n",
    "        layer_input = self.node_encoder(input)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(graph, layer_input)\n",
    "            if self.batch_norm:\n",
    "                hidden = self.batch_norm(hidden)\n",
    "            if self.activation:\n",
    "                hidden = self.activation(hidden)\n",
    "\n",
    "            if self.short_cut and hidden.shape == layer_input.shape:\n",
    "                hidden = hidden + layer_input\n",
    "            hiddens.append(hidden)\n",
    "            layer_input = hidden\n",
    "\n",
    "        if self.concat_hidden:\n",
    "            node_feature = torch.cat(hiddens, dim=-1)\n",
    "        else:\n",
    "            node_feature = hiddens[-1]\n",
    "        graph_feature = self.readout(graph, node_feature)\n",
    "\n",
    "        return {\n",
    "            'graph_feature': graph_feature,\n",
    "            'node_feature': node_feature\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "\n",
    "model = PNA(input_dim=dataset.node_feature_dim,\n",
    "                  hidden_dim=64, num_layer=3,\n",
    "                  edge_input_dim=None,\n",
    "                  num_relation=dataset.num_bond_type,\n",
    "                  aggregators=['mean'],\n",
    "                  scalers=['identity'],\n",
    "                  deg=deg, batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atom_type = dataset.num_atom_type\n",
    "# add one class for non-edge\n",
    "num_bond_type = dataset.num_bond_type + 1\n",
    "\n",
    "node_prior = distribution.IndependentGaussian(torch.zeros(num_atom_type),\n",
    "                                              torch.ones(num_atom_type))\n",
    "edge_prior = distribution.IndependentGaussian(torch.zeros(num_bond_type),\n",
    "                                              torch.ones(num_bond_type))\n",
    "node_flow = models.GraphAF(model, node_prior, num_layer=1)\n",
    "edge_flow = models.GraphAF(model, edge_prior, use_edge=True, num_layer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.AutoregressiveGeneration(node_flow, edge_flow,\n",
    "                                      max_node=38, max_edge_unroll=12,\n",
    "                                      task=\"plogp\", criterion=\"ppo\",\n",
    "                                      reward_temperature=1, baseline_momentum=0.9,\n",
    "                                      agent_update_interval=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:58:48   Preprocess training set\n",
      "01:58:50   {'batch_size': 64,\n",
      " 'class': 'core.Engine',\n",
      " 'gpus': None,\n",
      " 'gradient_interval': 1,\n",
      " 'log_interval': 10,\n",
      " 'logger': 'logging',\n",
      " 'num_worker': 0,\n",
      " 'optimizer': {'amsgrad': False,\n",
      "               'betas': (0.9, 0.999),\n",
      "               'class': 'optim.Adam',\n",
      "               'eps': 1e-08,\n",
      "               'lr': 0.0001,\n",
      "               'weight_decay': 0},\n",
      " 'scheduler': None,\n",
      " 'task': {'agent_update_interval': 5,\n",
      "          'baseline_momentum': 0.9,\n",
      "          'class': 'tasks.AutoregressiveGeneration',\n",
      "          'criterion': 'ppo',\n",
      "          'edge_model': {'class': 'models.GraphAF',\n",
      "                         'dequantization_noise': 0.9,\n",
      "                         'model': {'activation': 'relu',\n",
      "                                   'aggregators': ['mean'],\n",
      "                                   'batch_norm': True,\n",
      "                                   'class': 'PNA',\n",
      "                                   'concat_hidden': False,\n",
      "                                   'deg': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "                                   'divide_input': False,\n",
      "                                   'edge_input_dim': None,\n",
      "                                   'hidden_dim': 64,\n",
      "                                   'input_dim': 18,\n",
      "                                   'num_layer': 3,\n",
      "                                   'num_post_layer': 1,\n",
      "                                   'num_pre_layer': 1,\n",
      "                                   'num_relation': 3,\n",
      "                                   'num_tower': 1,\n",
      "                                   'readout': 'sum',\n",
      "                                   'scalers': ['identity'],\n",
      "                                   'short_cut': False},\n",
      "                         'num_layer': 1,\n",
      "                         'num_mlp_layer': 2,\n",
      "                         'prior': IndependentGaussian(),\n",
      "                         'use_edge': True},\n",
      "          'gamma': 0.9,\n",
      "          'max_edge_unroll': 12,\n",
      "          'max_node': 38,\n",
      "          'node_model': {'class': 'models.GraphAF',\n",
      "                         'dequantization_noise': 0.9,\n",
      "                         'model': {'activation': 'relu',\n",
      "                                   'aggregators': ['mean'],\n",
      "                                   'batch_norm': True,\n",
      "                                   'class': 'PNA',\n",
      "                                   'concat_hidden': False,\n",
      "                                   'deg': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "                                   'divide_input': False,\n",
      "                                   'edge_input_dim': None,\n",
      "                                   'hidden_dim': 64,\n",
      "                                   'input_dim': 18,\n",
      "                                   'num_layer': 3,\n",
      "                                   'num_post_layer': 1,\n",
      "                                   'num_pre_layer': 1,\n",
      "                                   'num_relation': 3,\n",
      "                                   'num_tower': 1,\n",
      "                                   'readout': 'sum',\n",
      "                                   'scalers': ['identity'],\n",
      "                                   'short_cut': False},\n",
      "                         'num_layer': 1,\n",
      "                         'num_mlp_layer': 2,\n",
      "                         'prior': IndependentGaussian(),\n",
      "                         'use_edge': False},\n",
      "          'num_edge_sample': -1,\n",
      "          'num_node_sample': -1,\n",
      "          'reward_temperature': 1,\n",
      "          'task': 'plogp'},\n",
      " 'test_set': None,\n",
      " 'train_set': {'atom_feature': 'symbol',\n",
      "               'class': 'datasets.ZINC250k',\n",
      "               'kekulize': True,\n",
      "               'path': '~/molecule-datasets/',\n",
      "               'verbose': 1},\n",
      " 'valid_set': None}\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(task.parameters(), lr=0.0001)\n",
    "solver = core.Engine(task, dataset, None, None, optimizer,\n",
    "                     batch_size=64, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:58:52   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "01:58:52   Epoch 0 begin\n",
      "01:58:54   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "01:58:54   Penalized logP: -4.39816\n",
      "01:58:54   Penalized logP (max): -1.7716\n",
      "01:58:54   edge PPO objective: 0.95427\n",
      "01:58:54   edge mask / graph: 1.70833\n",
      "01:58:54   node PPO objective: 0.469788\n",
      "01:58:54   node mask / graph: 1.95833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m solver\u001b[39m.\u001b[39;49mtrain(num_epoch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/graph-generative-models/torchdg0.2.0/torchdrug/core/engine.py:161\u001b[0m, in \u001b[0;36mEngine.train\u001b[0;34m(self, num_epoch, batch_per_epoch)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    159\u001b[0m     batch \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcuda(batch, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 161\u001b[0m loss, metric \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m loss\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLoss doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt require grad. Did you define any loss in the task?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/graph-generative-models/torchdg0.2.0/torchdrug/tasks/generation.py:124\u001b[0m, in \u001b[0;36mAutoregressiveGeneration.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    122\u001b[0m     metric\u001b[39m.\u001b[39mupdate(_metric)\n\u001b[1;32m    123\u001b[0m \u001b[39melif\u001b[39;00m criterion \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mppo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 124\u001b[0m     _loss, _metric \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreinforce_forward(batch)\n\u001b[1;32m    125\u001b[0m     all_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _loss \u001b[39m*\u001b[39m weight\n\u001b[1;32m    126\u001b[0m     metric\u001b[39m.\u001b[39mupdate(_metric)\n",
      "File \u001b[0;32m~/Desktop/graph-generative-models/torchdg0.2.0/torchdrug/tasks/generation.py:142\u001b[0m, in \u001b[0;36mAutoregressiveGeneration.reinforce_forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_id \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[39m# generation takes less time when early_stop=True\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m graph \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\u001b[39mlen\u001b[39;49m(batch[\u001b[39m\"\u001b[39;49m\u001b[39mgraph\u001b[39;49m\u001b[39m\"\u001b[39;49m]), off_policy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, early_stop\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(graph) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m graph\u001b[39m.\u001b[39mnum_nodes\u001b[39m.\u001b[39mmax() \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    144\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mGeneration results collapse to singleton molecules\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/graph-generative-models/torchdg0.2.0/torchdrug/tasks/generation.py:523\u001b[0m, in \u001b[0;36mAutoregressiveGeneration.generate\u001b[0;34m(self, num_sample, max_resample, off_policy, early_stop, verbose)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_resample):\n\u001b[1;32m    521\u001b[0m     \u001b[39m# only resample invalid graphs\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mis_valid\n\u001b[0;32m--> 523\u001b[0m     bond_pred[mask] \u001b[39m=\u001b[39m edge_model\u001b[39m.\u001b[39;49msample(graph, edge)[mask]\n\u001b[1;32m    524\u001b[0m     \u001b[39m# check valency\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     mask \u001b[39m=\u001b[39m (bond_pred \u001b[39m<\u001b[39m edge_model\u001b[39m.\u001b[39minput_dim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m~\u001b[39mcompleted\n",
      "File \u001b[0;32m~/Desktop/graph-generative-models/torchdg0.2.0/torchdrug/models/flow.py:107\u001b[0m, in \u001b[0;36mGraphAutoregressiveFlow.sample\u001b[0;34m(self, graph, edge, all_loss, metric)\u001b[0m\n\u001b[1;32m    104\u001b[0m edge \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_standarize_edge(graph, edge)\n\u001b[1;32m    106\u001b[0m node_feature \u001b[39m=\u001b[39m functional\u001b[39m.\u001b[39mone_hot(graph\u001b[39m.\u001b[39matom_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minput_dim)\n\u001b[0;32m--> 107\u001b[0m feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(graph, node_feature, all_loss, metric)\n\u001b[1;32m    108\u001b[0m node_feature \u001b[39m=\u001b[39m feature[\u001b[39m\"\u001b[39m\u001b[39mnode_feature\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    109\u001b[0m graph_feature \u001b[39m=\u001b[39m feature[\u001b[39m\"\u001b[39m\u001b[39mgraph_feature\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 294\u001b[0m, in \u001b[0;36mPNA.forward\u001b[0;34m(self, graph, input, all_loss, metric)\u001b[0m\n\u001b[1;32m    291\u001b[0m layer_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_encoder(\u001b[39minput\u001b[39m)\n\u001b[1;32m    293\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 294\u001b[0m     hidden \u001b[39m=\u001b[39m layer(graph, layer_input)\n\u001b[1;32m    295\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm:\n\u001b[1;32m    296\u001b[0m         hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm(hidden)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[51], line 195\u001b[0m, in \u001b[0;36mPNALayer.forward\u001b[0;34m(self, graph, input)\u001b[0m\n\u001b[1;32m    193\u001b[0m     update \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mcheckpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_and_aggregate, \u001b[39m*\u001b[39mgraph\u001b[39m.\u001b[39mto_tensors(), \u001b[39minput\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     update \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage_and_aggregate(graph, \u001b[39minput\u001b[39;49m)\n\u001b[1;32m    196\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine(\u001b[39minput\u001b[39m, update)\n\u001b[1;32m    197\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Desktop/graph-generative-models/torchdg0.2.0/torchdrug/layers/conv.py:61\u001b[0m, in \u001b[0;36mMessagePassingBase.message_and_aggregate\u001b[0;34m(self, graph, input)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mFused computation of message and aggregation over the graph.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mThis may provide better time or memory complexity than separate calls of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m    Tensor: node updates of shape :math:`(|V|, ...)`\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmessage(graph, \u001b[39minput\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m update \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(graph, message)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m update\n",
      "Cell \u001b[0;32mIn[51], line 174\u001b[0m, in \u001b[0;36mPNALayer.aggregate\u001b[0;34m(self, graph, message)\u001b[0m\n\u001b[1;32m    172\u001b[0m edge_weight \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medge_weight\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    173\u001b[0m edge_weight \u001b[39m=\u001b[39m edge_weight\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtowers, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m update \u001b[39m=\u001b[39m [aggr(message \u001b[39m*\u001b[39m edge_weight, node_out, graph\u001b[39m.\u001b[39mnum_node) \u001b[39mfor\u001b[39;00m aggr \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregators]\n\u001b[1;32m    175\u001b[0m update \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(update, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    176\u001b[0m deg \u001b[39m=\u001b[39m degree(node_out, graph\u001b[39m.\u001b[39mnum_node, dtype\u001b[39m=\u001b[39mmessage\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[51], line 174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m edge_weight \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medge_weight\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    173\u001b[0m edge_weight \u001b[39m=\u001b[39m edge_weight\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtowers, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 174\u001b[0m update \u001b[39m=\u001b[39m [aggr(message \u001b[39m*\u001b[39;49m edge_weight, node_out, graph\u001b[39m.\u001b[39;49mnum_node) \u001b[39mfor\u001b[39;00m aggr \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregators]\n\u001b[1;32m    175\u001b[0m update \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(update, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    176\u001b[0m deg \u001b[39m=\u001b[39m degree(node_out, graph\u001b[39m.\u001b[39mnum_node, dtype\u001b[39m=\u001b[39mmessage\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[51], line 20\u001b[0m, in \u001b[0;36maggregate_mean\u001b[0;34m(src, index, dim_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate_mean\u001b[39m(src: Tensor, index: Tensor, dim_size: Optional[\u001b[39mint\u001b[39m]):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter(src, index, \u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m, dim_size, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch_scatter/scatter.py:156\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_mul(src, index, dim, out, dim_size)\n\u001b[1;32m    155\u001b[0m \u001b[39melif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_mean(src, index, dim, out, dim_size)\n\u001b[1;32m    157\u001b[0m \u001b[39melif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m scatter_min(src, index, dim, out, dim_size)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch_scatter/scatter.py:41\u001b[0m, in \u001b[0;36mscatter_mean\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatter_mean\u001b[39m(src: torch\u001b[39m.\u001b[39mTensor, index: torch\u001b[39m.\u001b[39mTensor, dim: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     39\u001b[0m                  out: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m                  dim_size: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 41\u001b[0m     out \u001b[39m=\u001b[39m scatter_sum(src, index, dim, out, dim_size)\n\u001b[1;32m     42\u001b[0m     dim_size \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39msize(dim)\n\u001b[1;32m     44\u001b[0m     index_dim \u001b[39m=\u001b[39m dim\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch_scatter/scatter.py:20\u001b[0m, in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         size[dim] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(size, dtype\u001b[39m=\u001b[39;49msrc\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49msrc\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     22\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "solver.train(num_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.GearNet(input_dim=dataset.node_feature_dim,\n",
    "                    num_relation=dataset.num_bond_type,\n",
    "                    hidden_dims=[64, 64, 64], batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import degree\n",
    "from pna import PNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = torch.zeros(10, dtype=torch.long)\n",
    "for data in dataset:\n",
    "    graph = data['graph']\n",
    "    d = degree(graph.edge_list[:, 1], num_nodes=graph.num_node, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "PNA(input_dim=dataset.node_feature_dim,\n",
    "                  hidden_dim=64, num_layer=3,\n",
    "                  edge_input_dim=dataset.edge_feature_dim,\n",
    "                  num_relation=dataset.num_bond_type,\n",
    "                  aggregators=['mean'],\n",
    "                  scalers=['identity'],\n",
    "                  deg=deg, batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsn import GSN\n",
    "from gsn import prepare_GSN_dataset\n",
    "prepare_GSN_dataset(dataset, max_cycle=8)\n",
    "\n",
    "GSN(input_dim=dataset.node_feature_dim,\n",
    "                  hidden_dim=64, num_layer=3,\n",
    "                  edge_input_dim=dataset.edge_feature_dim,\n",
    "                  num_relation=dataset.num_bond_type,\n",
    "                  batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import core, tasks\n",
    "\n",
    "model = GATV2(input_dim=dataset.node_feature_dim,\n",
    "                    num_relation=dataset.num_bond_type,\n",
    "                    hidden_dims=[256, 256, 256, 256], batch_norm=True)\n",
    "task = tasks.GCPNGeneration(model, dataset.atom_types, max_edge_unroll=12,\n",
    "                            max_node=38, criterion=\"nll\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
