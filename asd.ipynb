{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /Users/ca/molecule-datasets/250k_rndm_zinc_drugs_clean_3.csv:  50%|█████     | 249456/498911 [00:03<00:03, 63299.98it/s]\n",
      "Constructing molecules from SMILES: 100%|██████████| 249455/249455 [05:35<00:00, 743.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchdrug import datasets\n",
    "\n",
    "dataset = datasets.ZINC250k(\"~/molecule-datasets/\", kekulize=True,\n",
    "                            atom_feature=\"symbol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import core, models, tasks\n",
    "\n",
    "model = models.GIN(input_dim=dataset.node_feature_dim,\n",
    "                    num_relation=dataset.num_bond_type,\n",
    "                    hidden_dims=[256, 256, 256, 256], batch_norm=False)\n",
    "task = tasks.GCPNGeneration(model, dataset.atom_types, max_edge_unroll=12,\n",
    "                            max_node=38, criterion=\"nll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:29:19   Preprocess training set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:29:19   {'batch_size': 128,\n",
      " 'class': 'core.Engine',\n",
      " 'gpus': None,\n",
      " 'gradient_interval': 1,\n",
      " 'log_interval': 10,\n",
      " 'logger': 'logging',\n",
      " 'num_worker': 0,\n",
      " 'optimizer': {'amsgrad': False,\n",
      "               'betas': (0.9, 0.999),\n",
      "               'class': 'optim.Adam',\n",
      "               'eps': 1e-08,\n",
      "               'lr': 0.001,\n",
      "               'weight_decay': 0},\n",
      " 'scheduler': None,\n",
      " 'task': {'agent_update_interval': 10,\n",
      "          'atom_types': [6, 7, 8, 9, 15, 16, 17, 35, 53],\n",
      "          'baseline_momentum': 0.9,\n",
      "          'class': 'tasks.GCPNGeneration',\n",
      "          'criterion': 'nll',\n",
      "          'gamma': 0.9,\n",
      "          'hidden_dim_mlp': 128,\n",
      "          'max_edge_unroll': 12,\n",
      "          'max_node': 38,\n",
      "          'model': {'activation': 'relu',\n",
      "                    'batch_norm': False,\n",
      "                    'class': 'models.GAT',\n",
      "                    'concat_hidden': False,\n",
      "                    'edge_input_dim': None,\n",
      "                    'hidden_dims': [256, 256, 256, 256],\n",
      "                    'input_dim': 18,\n",
      "                    'negative_slope': 0.2,\n",
      "                    'num_head': 1,\n",
      "                    'num_relation': 3,\n",
      "                    'readout': 'sum',\n",
      "                    'short_cut': False},\n",
      "          'reward_temperature': 1,\n",
      "          'task': ()},\n",
      " 'test_set': None,\n",
      " 'train_set': {'atom_feature': 'symbol',\n",
      "               'class': 'datasets.ZINC250k',\n",
      "               'kekulize': True,\n",
      "               'path': '~/molecule-datasets/',\n",
      "               'verbose': 1},\n",
      " 'valid_set': None}\n",
      "16:29:19   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "16:29:19   Epoch 0 begin\n",
      "16:29:42   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "16:29:42   edge acc: 0.446082\n",
      "16:29:42   edge loss: 1.06519\n",
      "16:29:42   node1 acc: 0.224765\n",
      "16:29:42   node1 loss: 2.2678\n",
      "16:29:42   node2 acc: 0.023511\n",
      "16:29:42   node2 loss: 2.9517\n",
      "16:29:42   stop acc: 0.57173\n",
      "16:29:42   stop bce loss: 0.736438\n",
      "16:29:42   total loss: 7.02112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(task\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m)\n\u001b[1;32m      3\u001b[0m solver \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mEngine(task, dataset, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, optimizer,\n\u001b[1;32m      4\u001b[0m                      batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, log_interval\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m solver\u001b[39m.\u001b[39;49mtrain(num_epoch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/graph-generative-models/torchdg0.2.0/torchdrug/core/engine.py:165\u001b[0m, in \u001b[0;36mEngine.train\u001b[0;34m(self, num_epoch, batch_per_epoch)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLoss doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt require grad. Did you define any loss in the task?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m gradient_interval\n\u001b[0;32m--> 165\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    166\u001b[0m metrics\u001b[39m.\u001b[39mappend(metric)\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m batch_id \u001b[39m-\u001b[39m start_id \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m gradient_interval:\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    148\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    149\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "optimizer = optim.Adam(task.parameters(), lr = 1e-3)\n",
    "solver = core.Engine(task, dataset, None, None, optimizer,\n",
    "                     batch_size=128, log_interval=10)\n",
    "\n",
    "solver.train(num_epoch=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
